# -*- coding: utf-8 -*-
"""
Created on Tue Dec 13 10:07:55 2022

@author: Ittipat
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

#data=pd.read_csv('C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\data\\SLC6A_active_excape_export2.csv')
data=pd.read_csv('C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\data\\LC_Pre_plated_Bioactive_Compound_Library_8000cmpds_384w.csv')
#data=pd.read_csv('/home/ittipat/ligands/LC_Pre_plated_Bioactive_Compound_Library_8000cmpds_384w.csv')
plt.hist(data['molLD50'], density=True, bins=30) # density=False would make counts
plt.ylabel('LD50')
plt.xlabel('Data')

data.head(1)

#plot histogram


from rdkit import Chem, DataStructs
from rdkit.Chem import PandasTools, AllChem
PandasTools.AddMoleculeColumnToFrame(data,'smiles','Molecule')
data[["smiles","Molecule"]].head(1)

data.Molecule.isna().sum()

isna = data.Molecule.isna()

for i in range(len(data)):
    if isna[i]==True:
        data=data.drop(i)
        
data.Molecule.isna().sum()


def mol2fp(mol):
    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)
    ar = np.zeros((1,), dtype=np.int8)
    DataStructs.ConvertToNumpyArray(fp, ar)
    return ar

fp =mol2fp(Chem.MolFromSmiles(data.loc[1,"smiles"]))
plt.matshow(fp.reshape((64,-1)), cmap=plt.get_cmap('Greys'), fignum=1)


	
data["FPs"] = data.Molecule.apply(mol2fp)
X = np.stack(data.FPs.values)
print(X.shape)

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

y = data.molLD50.values.reshape((-1,1))
X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.20)
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train,  test_size=0.1)
#Normalizing output using standard scaling
scaler = StandardScaler()
y_train = scaler.fit_transform(y_train)
y_test = scaler.transform(y_test)
y_validation = scaler.transform(y_validation)

# We'll remove low variance features
from sklearn.feature_selection import VarianceThreshold
feature_select = VarianceThreshold(threshold=0.05)
X_train = feature_select.fit_transform(X_train)
X_validation = feature_select.transform(X_validation)
X_test = feature_select.transform(X_test)
X_train.shape

# Let's get those arrays transfered to the GPU memory as tensors
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
# If you don't have a GPU, buy a graphics card. I have for a long time used a 1060 GTX, which is not that expensive anymore.
X_train = torch.tensor(X_train, device=device).float()
X_test = torch.tensor(X_test, device=device).float()
X_validation = torch.tensor(X_validation, device=device).float()
y_train = torch.tensor(y_train, device=device).float()
y_test = torch.tensor(y_test, device=device).float()
y_validation = torch.tensor(y_validation, device=device).float()
X_train

	
X_train.shape


from torch.utils.data import TensorDataset
train_dataset = TensorDataset(X_train, y_train)
validation_dataset = TensorDataset(X_validation, y_validation)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=256,
                                          shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,
                                          batch_size=256,
                                          shuffle=False)

class Net(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_rate, out_size):
        super(Net, self).__init__()
        # Three layers and a output layer
        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc_out = nn.Linear(hidden_size, out_size) # Output layer
        #Layer normalization for faster training
        self.ln0 = nn.LayerNorm(input_size)  #try normalization input
        self.ln1 = nn.LayerNorm(hidden_size)
        self.ln2 = nn.LayerNorm(hidden_size)
        self.ln3 = nn.LayerNorm(hidden_size)        
        #LeakyReLU will be used as the activation function
        self.activation = nn.LeakyReLU()
        #Dropout for regularization
        self.dropout = nn.Dropout(dropout_rate)
     
    def forward(self, x):# Forward pass: stacking each layer together
        # Fully connected =&amp;gt; Layer Norm =&amp;gt; LeakyReLU =&amp;gt; Dropout times 3
        out = self.ln0(x)
        out = self.fc1(out)
        out = self.ln1(out)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.fc2(out)
        out = self.ln2(out)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.fc3(out)
        out = self.ln3(out)
        out = self.activation(out)
        out = self.dropout(out)
        #Final output layer
        out = self.fc_out(out)
        return out
    
#Defining the hyperparameters
input_size = X_train.size()[-1]     # The input size should fit our fingerprint size
hidden_size = 256  # The size of the hidden layer
dropout_rate = 0.80    # The dropout rate
output_size = 1        # This is just a single task, so this will be one
learning_rate = 0.0001  # The learning rate for the optimizer
model = Net(input_size, hidden_size, dropout_rate, output_size)

#model.cuda()

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = torch.nn.MSELoss()

model.train() #Ensure the network is in "train" mode with dropouts active
epochs = 5
train_losses = []
val_losses = []
for e in range(epochs):
    running_loss = 0
    for fps, labels in train_loader:
        # Training pass
        optimizer.zero_grad() # Initialize the gradients, which will be recorded during the forward pa
         
        output = model(fps) #Forward pass of the mini-batch
        loss = criterion(output, labels) #Computing the loss
        loss.backward() # calculate the backward pass
        optimizer.step() # Optimize the weights
         
        #save training loss
        running_loss += loss.item()
        train_losses.append(loss.item())
        # Compute the validation loss
        val_loss = loss_fn(model(X_validation), y_validation)
    
        # Save the validation loss
        val_losses.append(val_loss.item())
        
    validation_loss = torch.mean(( y_validation - model(X_validation) )**2).item()
        
    print("Epoch: %3i Training loss: %0.2F Validation loss: %0.2F"%(e,(running_loss/len(train_loader)), validation_loss))


#save model and optimizer
torch.save(model.state_dict(), "C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\model\\model_size256_epoch1500.pt")
torch.save(optimizer.state_dict(), "C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\model\\optimizer_size256_epoch1500.pt")

# Load the model

model = torch.load("C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\model\\model_size256_epoch1500.pt")

# Load the optimizer

optimizer = torch.load("C:\\Users\\Ittipat\\OneDrive - UC San Diego\\dnetw2v\\model\\optimizer_size256_epoch1500.pt")

#plot loss
plt.plot(train_losses, label="Training loss")
plt.plot(val_losses, label="Val loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

model.eval() #Swith to evaluation mode, where dropout is switched off
y_pred_train = model(X_train)
y_pred_validation = model(X_validation)
y_pred_test = model(X_test)

torch.mean(( y_train - y_pred_train )**2).item()
torch.mean(( y_validation - y_pred_validation )**2).item()
torch.mean(( y_test - y_pred_test )**2).item()

def flatten(tensor):
    return tensor.cpu().detach().numpy().flatten()

plt.ylim(-4,4)
plt.scatter(flatten(y_pred_test), flatten(y_test), alpha=0.5, label="Test")
plt.scatter(flatten(y_pred_train), flatten(y_train), alpha=0.05, label="Train")
plt.legend()
plt.plot([-1.0, 5.5], [-0.5,5.5], c="b")

plt.ylim(-4,4)
plt.scatter(flatten(y_pred_test), flatten(y_test), alpha=0.25, label="Test")
plt.legend()
plt.plot([-1.0, 5.5], [-0.5,5.5], c="b")

test_loss = torch.mean(( y_test - model(X_test) )**2).item()
train_loss = torch.mean(( y_train - model(X_train) )**2).item()
test_loss = torch.mean(( y_test - model(X_test) )**2).item()

def predict_smiles(smiles):
    fp =mol2fp(Chem.MolFromSmiles(smiles)).reshape(1,-1)
    fp_filtered = feature_select.transform(fp)
    fp_tensor = torch.tensor(fp_filtered, device=device).float()
    prediction = model(fp_tensor)
    #return prediction.cpu().detach().numpy()
    pXC50 = scaler.inverse_transform(prediction.cpu().detach().numpy())
    return pXC50[0][0]
predict_smiles('Cc1ccc2c(N3CCNCC3)cc(F)cc2n1')
